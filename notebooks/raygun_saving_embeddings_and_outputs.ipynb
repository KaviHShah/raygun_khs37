{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "643f4338-3ea2-4905-b047-da4cc379bb69",
   "metadata": {},
   "source": [
    "#This script is to run raygun to see what esm2 mask replacement and raygun fixed length embeddings to form final model do when partial sequences are run.\n",
    "\n",
    "Write functions to run step by step parts of the raygun model and save the intermediates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cf0c62f-9a96-4f85-8d55-6e86af0f3c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA RTX A6000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())   # Should be True if CUDA works\n",
    "print(torch.cuda.device_count())   # Number of GPUs\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74f8c867-7b59-401e-94f3-bfaa323099f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ESM2(\n",
       "  (embed_tokens): Embedding(33, 1280, padding_idx=1)\n",
       "  (layers): ModuleList(\n",
       "    (0-32): 33 x TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (contact_head): ContactPredictionHead(\n",
       "    (regression): Linear(in_features=660, out_features=1, bias=True)\n",
       "    (activation): Sigmoid()\n",
       "  )\n",
       "  (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# esm-2 model\n",
    "from esm.pretrained import esm2_t33_650M_UR50D\n",
    "esmmodel, alph = esm2_t33_650M_UR50D()\n",
    "bc             = alph.get_batch_converter()\n",
    "esmmodel       = esmmodel.to(0)\n",
    "esmmodel.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c33ec846-fe27-404d-ba8d-c7165923aea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data is a list of (id,sequence) pairs\n",
    "data = [(\"egfp\", \"MVSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLTYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITLGMDELYK\")]\n",
    "_, _, tok = bc(data)\n",
    "# return esmemb\n",
    "esmemb       = esmmodel(tok.to(0), repr_layers = [33],\n",
    "                        return_contacts=False)[\"representations\"][33][:,1:-1]  #remember to remove the start and end tokens!\n",
    "# esmemb.shape should be torch.Size([1, 239, 1280])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d6990c0-4241-4234-a1be-8312aef8a294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 239, 1280])\n",
      "239\n",
      "torch.Size([1, 241])\n"
     ]
    }
   ],
   "source": [
    "print(esmemb.shape)\n",
    "#length of the sequence\n",
    "print(len(data[0][1]))\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "#plt.imshow(esmemb.cpu().detach().numpy()[0])\n",
    "print(tok.shape)\n",
    "#plt.imshow(tok.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a37c12a-2ec2-48fc-b290-72dea6ba05c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /hpc/home/khs36/.cache/torch/hub/rohitsinghlab_raygun_main\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Raygun(\n",
       "  (encoder): RaygunEncoder(\n",
       "    (encoders): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (encoder): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): ESM1LayerNorm()\n",
       "          (fc1): Linear(in_features=1280, out_features=2560, bias=True)\n",
       "          (fc2): Linear(in_features=2560, out_features=1280, bias=True)\n",
       "          (final_layer_norm): ESM1LayerNorm()\n",
       "        )\n",
       "        (convblock): ConvBlock(\n",
       "          (c1): ConvMasked(\n",
       "            (conv): Conv1d(1280, 640, kernel_size=(7,), stride=(1,), padding=valid)\n",
       "          )\n",
       "          (s1): SiLU()\n",
       "          (c2): ConvMasked(\n",
       "            (conv): Conv1d(640, 320, kernel_size=(3,), stride=(1,), padding=valid)\n",
       "          )\n",
       "          (s2): SiLU()\n",
       "          (c3): ConvMasked(\n",
       "            (conv): Conv1d(320, 640, kernel_size=(7,), stride=(1,), padding=valid)\n",
       "          )\n",
       "          (s3): SiLU()\n",
       "        )\n",
       "        (final): Linear(in_features=640, out_features=1280, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (reduction): Reduction()\n",
       "    (final): Sequential(\n",
       "      (0): Linear(in_features=16640, out_features=392, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=392, out_features=1280, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): RaygunDecoder(\n",
       "    (dbefore): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (encoder): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): ESM1LayerNorm()\n",
       "          (fc1): Linear(in_features=1280, out_features=2560, bias=True)\n",
       "          (fc2): Linear(in_features=2560, out_features=1280, bias=True)\n",
       "          (final_layer_norm): ESM1LayerNorm()\n",
       "        )\n",
       "        (convblock): ConvBlock(\n",
       "          (c1): ConvMasked(\n",
       "            (conv): Conv1d(1280, 640, kernel_size=(7,), stride=(1,), padding=valid)\n",
       "          )\n",
       "          (s1): SiLU()\n",
       "          (c2): ConvMasked(\n",
       "            (conv): Conv1d(640, 320, kernel_size=(3,), stride=(1,), padding=valid)\n",
       "          )\n",
       "          (s2): SiLU()\n",
       "          (c3): ConvMasked(\n",
       "            (conv): Conv1d(320, 640, kernel_size=(7,), stride=(1,), padding=valid)\n",
       "          )\n",
       "          (s3): SiLU()\n",
       "        )\n",
       "        (final): Linear(in_features=640, out_features=1280, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (repetition): Repetition()\n",
       "    (dafter): ModuleList(\n",
       "      (0-12): 13 x Block(\n",
       "        (encoder): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): ESM1LayerNorm()\n",
       "          (fc1): Linear(in_features=1280, out_features=2560, bias=True)\n",
       "          (fc2): Linear(in_features=2560, out_features=1280, bias=True)\n",
       "          (final_layer_norm): ESM1LayerNorm()\n",
       "        )\n",
       "        (convblock): ConvBlock(\n",
       "          (c1): ConvMasked(\n",
       "            (conv): Conv1d(1280, 640, kernel_size=(7,), stride=(1,), padding=valid)\n",
       "          )\n",
       "          (s1): SiLU()\n",
       "          (c2): ConvMasked(\n",
       "            (conv): Conv1d(640, 320, kernel_size=(3,), stride=(1,), padding=valid)\n",
       "          )\n",
       "          (s2): SiLU()\n",
       "          (c3): ConvMasked(\n",
       "            (conv): Conv1d(320, 640, kernel_size=(7,), stride=(1,), padding=valid)\n",
       "          )\n",
       "          (s3): SiLU()\n",
       "        )\n",
       "        (final): Linear(in_features=640, out_features=1280, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (final): Sequential(\n",
       "      (0): Linear(in_features=17920, out_features=364, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=364, out_features=1280, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (esmdecoder): DecoderBlock(\n",
       "    (encoder): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): ESM1LayerNorm()\n",
       "      (fc1): Linear(in_features=1280, out_features=2560, bias=True)\n",
       "      (fc2): Linear(in_features=2560, out_features=1280, bias=True)\n",
       "      (final_layer_norm): ESM1LayerNorm()\n",
       "    )\n",
       "    (final): Sequential(\n",
       "      (0): Linear(in_features=1280, out_features=320, bias=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): Linear(in_features=320, out_features=32, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from raygun.pretrained import raygun_4_4mil_800M\n",
    "\n",
    "# Load the bare PyTorch model (not Lightning)\n",
    "raymodel = raygun_4_4mil_800M(return_lightning_module=False)\n",
    "\n",
    "# Print the model architecture\n",
    "raymodel.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6c77ff-9a80-44f5-a526-5a129ccc299f",
   "metadata": {},
   "source": [
    "#Here to put the model specification code: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d0b8fc8-7165-4b9c-b255-53970ee82937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Raygun(\n",
       "  (encoder): RaygunEncoder(\n",
       "    (encoders): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (encoder): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): ESM1LayerNorm()\n",
       "          (fc1): Linear(in_features=1280, out_features=2560, bias=True)\n",
       "          (fc2): Linear(in_features=2560, out_features=1280, bias=True)\n",
       "          (final_layer_norm): ESM1LayerNorm()\n",
       "        )\n",
       "        (convblock): ConvBlock(\n",
       "          (c1): ConvMasked(\n",
       "            (conv): Conv1d(1280, 640, kernel_size=(7,), stride=(1,), padding=valid)\n",
       "          )\n",
       "          (s1): SiLU()\n",
       "          (c2): ConvMasked(\n",
       "            (conv): Conv1d(640, 320, kernel_size=(3,), stride=(1,), padding=valid)\n",
       "          )\n",
       "          (s2): SiLU()\n",
       "          (c3): ConvMasked(\n",
       "            (conv): Conv1d(320, 640, kernel_size=(7,), stride=(1,), padding=valid)\n",
       "          )\n",
       "          (s3): SiLU()\n",
       "        )\n",
       "        (final): Linear(in_features=640, out_features=1280, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (reduction): Reduction()\n",
       "    (final): Sequential(\n",
       "      (0): Linear(in_features=16640, out_features=392, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=392, out_features=1280, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): RaygunDecoder(\n",
       "    (dbefore): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (encoder): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): ESM1LayerNorm()\n",
       "          (fc1): Linear(in_features=1280, out_features=2560, bias=True)\n",
       "          (fc2): Linear(in_features=2560, out_features=1280, bias=True)\n",
       "          (final_layer_norm): ESM1LayerNorm()\n",
       "        )\n",
       "        (convblock): ConvBlock(\n",
       "          (c1): ConvMasked(\n",
       "            (conv): Conv1d(1280, 640, kernel_size=(7,), stride=(1,), padding=valid)\n",
       "          )\n",
       "          (s1): SiLU()\n",
       "          (c2): ConvMasked(\n",
       "            (conv): Conv1d(640, 320, kernel_size=(3,), stride=(1,), padding=valid)\n",
       "          )\n",
       "          (s2): SiLU()\n",
       "          (c3): ConvMasked(\n",
       "            (conv): Conv1d(320, 640, kernel_size=(7,), stride=(1,), padding=valid)\n",
       "          )\n",
       "          (s3): SiLU()\n",
       "        )\n",
       "        (final): Linear(in_features=640, out_features=1280, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (repetition): Repetition()\n",
       "    (dafter): ModuleList(\n",
       "      (0-12): 13 x Block(\n",
       "        (encoder): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): ESM1LayerNorm()\n",
       "          (fc1): Linear(in_features=1280, out_features=2560, bias=True)\n",
       "          (fc2): Linear(in_features=2560, out_features=1280, bias=True)\n",
       "          (final_layer_norm): ESM1LayerNorm()\n",
       "        )\n",
       "        (convblock): ConvBlock(\n",
       "          (c1): ConvMasked(\n",
       "            (conv): Conv1d(1280, 640, kernel_size=(7,), stride=(1,), padding=valid)\n",
       "          )\n",
       "          (s1): SiLU()\n",
       "          (c2): ConvMasked(\n",
       "            (conv): Conv1d(640, 320, kernel_size=(3,), stride=(1,), padding=valid)\n",
       "          )\n",
       "          (s2): SiLU()\n",
       "          (c3): ConvMasked(\n",
       "            (conv): Conv1d(320, 640, kernel_size=(7,), stride=(1,), padding=valid)\n",
       "          )\n",
       "          (s3): SiLU()\n",
       "        )\n",
       "        (final): Linear(in_features=640, out_features=1280, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (final): Sequential(\n",
       "      (0): Linear(in_features=17920, out_features=364, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=364, out_features=1280, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (esmdecoder): DecoderBlock(\n",
       "    (encoder): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): ESM1LayerNorm()\n",
       "      (fc1): Linear(in_features=1280, out_features=2560, bias=True)\n",
       "      (fc2): Linear(in_features=2560, out_features=1280, bias=True)\n",
       "      (final_layer_norm): ESM1LayerNorm()\n",
       "    )\n",
       "    (final): Sequential(\n",
       "      (0): Linear(in_features=1280, out_features=320, bias=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): Linear(in_features=320, out_features=32, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c468fd-df6d-41e1-9104-c97e2db5e17c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raygun",
   "language": "python",
   "name": "raygun"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
